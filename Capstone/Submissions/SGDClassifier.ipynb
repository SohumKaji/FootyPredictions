{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Classifier Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Fixing Function for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gives a score based on how much temp_name matches each name in namelist\n",
    "#The score is simply +1 for each consecutive matching letter\n",
    "#The longest length of consecutive matching letters is the score for each name in namelist\n",
    "def name_fix(temp_name, namelist):\n",
    "\n",
    "    #Special case:\n",
    "    if(temp_name == 'EspanyolBarcelona'): return 'RCDEspanyol'\n",
    "    if(temp_name == 'ChievoVerona'): return 'Chievo'\n",
    "    if(temp_name == 'HellasVerona'): return 'Verona'\n",
    "    \n",
    "    score = []\n",
    "    for elem in namelist:\n",
    "        count = 0\n",
    "        i = 0\n",
    "        curr_word = ''\n",
    "        letters = list(temp_name)\n",
    "        \n",
    "        #While the score is less than the length of the word in question\n",
    "        while i < len(letters):\n",
    "            #Increase the segment you are checking for matches by 1 letter\n",
    "            curr_word += letters[i]\n",
    "            \n",
    "            #If the new segment is in the name from namelist and it is longer than the\n",
    "            #max segment so far, store this as the maximum\n",
    "            if((curr_word in elem)): \n",
    "                if(count < len(curr_word)): \n",
    "                    count = len(curr_word)\n",
    "                    \n",
    "            #If the segment is not contained in the name, reset\n",
    "            elif(len(curr_word) != 1): \n",
    "                curr_word = ''\n",
    "                i-=1\n",
    "            \n",
    "            i+=1    \n",
    "\n",
    "        score.append(count)\n",
    "        \n",
    "    #The name with the highest score is returned    \n",
    "    return namelist[np.argmax(score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in & Initialize all of the required data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "home_path = 'sgd_home.csv'\n",
    "away_path = 'sgd_away.csv'\n",
    "\n",
    "#Read in fulltime match stats\n",
    "home_df = pd.read_csv(home_path, index_col=0)\n",
    "away_df = pd.read_csv(away_path, index_col=0)\n",
    "\n",
    "#Read in match information\n",
    "with open('match_dates_and_scores.pickle', 'rb') as f:\n",
    "    results, match_dates = pickle.load(f)\n",
    "\n",
    "count, total = 0, 0\n",
    "x, y, X_train, X_test, y_train, y_test, half_time = [], [], [] ,[], [], [], []\n",
    "\n",
    "#Set train/test split to 80/20\n",
    "test_train_limit = match_dates[0]+ int((match_dates[-1]-match_dates[0])*(4/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Name Fixing function above to clean up the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['SCBastia', 'OlympiqueMarseille', (1, 2), (3, 3)], ['ÉvianThononGaillard', 'SMCaen', (0, 3), (0, 3)], ['EAGuingamp', 'ASSaint-Étienne', (0, 1), (0, 2)], ['LilleOSC', 'FCMetz', (0, 0), (0, 0)], ['MontpellierHSC', 'GirondinsBordeaux', (0, 1), (0, 1)], ['FCNantes', 'RCLens', (0, 0), (1, 0)], ['OGCNice', 'ToulouseFC', (1, 2), (3, 2)]]\n",
      "[['Bastia', 'Marseille', (1, 2), (3, 3)], ['EvianThononGaillard', 'Caen', (0, 3), (0, 3)], ['Guingamp', 'SaintEtienne', (0, 1), (0, 2)], ['Lille', 'Metz', (0, 0), (0, 0)], ['Montpellier', 'Bordeaux', (0, 1), (0, 1)], ['Nantes', 'Lens', (0, 0), (1, 0)], ['Nice', 'Toulouse', (1, 2), (3, 2)]]\n"
     ]
    }
   ],
   "source": [
    "#Get list of team names according to the fulltime data\n",
    "match_id_list_home = list(home_df.index)\n",
    "match_id_list_away = list(away_df.index)\n",
    "match_id_list_home = list(set([elem.split('_')[0] for elem in match_id_list_home]))\n",
    "match_id_list_away = list(set([elem.split('_')[0] for elem in match_id_list_away]))\n",
    "\n",
    "#Un-fixed names to verify visually\n",
    "print(results[5331])\n",
    "\n",
    "#For each date where a game was played\n",
    "for date in match_dates:\n",
    "    #For each match played that day\n",
    "    for match in results[date]:\n",
    "            #Replace the name in the schedule with the equivalent name in the fulltime data\n",
    "            match[0] = name_fix(match[0], match_id_list_home)\n",
    "            match[1] = name_fix(match[1], match_id_list_away)\n",
    "\n",
    "#Save our fixed data\n",
    "with open('fixed_match_dates_and_scores.pkl', 'wb') as fid:\n",
    "    pickle.dump((results, match_dates), fid)    \n",
    "\n",
    "#Fixed names to verify visually\n",
    "print(results[5331])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in from the Dataframes, fill X_train, X_test, y_train, y_test with the appropriate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3554 1482 3554 1482\n"
     ]
    }
   ],
   "source": [
    "#For each date where a game was played\n",
    "for date in match_dates:\n",
    "    #For each match played that day\n",
    "    for match in results[date]:\n",
    "        \n",
    "        #Initialize for each match\n",
    "        temp_vec = 0\n",
    "        flag = [False, False]\n",
    "        home_name = match[0]\n",
    "        away_name = match[1]\n",
    "\n",
    "        #Only look for data from prior to the match date for the Home team\n",
    "        #This way we do not use data from the match in our prediction of the match\n",
    "        temp_date = date - 1\n",
    "\n",
    "        #Iterate through the previous match days until you find a prior set of fulltime data\n",
    "        #In this dataframe the fulltime data is a rolling average of the previous fulltime data in the same season\n",
    "        while(temp_date > match_dates[0]):\n",
    "            #if there is data for the home team on temp_date day then load it\n",
    "            try:\n",
    "                temp_vec = [[float(elem) for elem in (home_df.loc[home_name+'_'+str(temp_date), :]).as_matrix()]]\n",
    "                \n",
    "                #When we find the data, indicate that we have found it and exit this loop\n",
    "                flag[0] = True\n",
    "                break\n",
    "            #if there is no data, check the day before\n",
    "            except:\n",
    "                temp_date -=1\n",
    "                \n",
    "\n",
    "        #Reset and Reuse\n",
    "        temp_date = date -1\n",
    "\n",
    "        #Identical to above but for the Away Team\n",
    "        while(temp_date > match_dates[0] and flag[0] == True): \n",
    "            try:    \n",
    "                temp_vec.append([float(elem) for elem in away_df.loc[away_name+'_'+str(temp_date), :].as_matrix()])\n",
    "                flag[1] = True\n",
    "                break\n",
    "            except:\n",
    "                temp_date -=1\n",
    "                \n",
    "        #If we have found prior data for both teams (we won't for the first 2 weeks of our data)\n",
    "        #In addition, 3 teams in each league each year are newly promoted from the 2nd division\n",
    "        #they will have no prior data\n",
    "        if(flag[0] and flag[1]):\n",
    "            #Since match data is chronologically ordered, the earlier matches will be placed in \n",
    "            #the train category and the later matches in the test category\n",
    "            if(date < test_train_limit):\n",
    "                X_train.append(temp_vec)\n",
    "                y_train.append(list(match[3]))\n",
    "            else:\n",
    "                X_test.append(temp_vec)\n",
    "                y_test.append(list(match[3]))\n",
    "                \n",
    "                #Beginning creation of the Benchmark\n",
    "                half_time.append(list(match[2]))\n",
    "                \n",
    "                \n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Save the SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Shape: [[  32.    7.   14.    3.   10.    2.   nan   20.    9.    6.    1.   19.\n",
      "     7.   21.  343.   78.]\n",
      " [  60.   14.   21.   11.    7.    7.    0.   33.   22.    3.    3.   21.\n",
      "    13.   25.  504.   85.]]\n",
      "Re-Shape: [  32.    7.   14.    3.   10.    2.    0.   20.    9.    6.    1.   19.\n",
      "    7.   21.  343.   78.   60.   14.   21.   11.    7.    7.    0.   33.\n",
      "   22.    3.    3.   21.   13.   25.  504.   85.]\n",
      "Pre-Binarizing: [1 2]\n",
      "Post-Binarizing: [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "1482\n"
     ]
    }
   ],
   "source": [
    "#Convert the data into numpy arrays\n",
    "X_train, X_test = np.array(X_train), np.array(X_test) \n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "half_time = np.array(half_time)\n",
    "\n",
    "#Classifier Replace all NaN Values with 0\n",
    "#This makes sense because some values (eg: red cards or yellow cards) when scraped were represented with \n",
    "#empty space rather than a 0\n",
    "print(\"Pre-Shape:\", X_train[0])\n",
    "X_train = np.nan_to_num(X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.nan_to_num(X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "print(\"Re-Shape:\", X_train[0])\n",
    "\n",
    "#MultilabelBinarizer to convert scores into a vector\n",
    "print(\"Pre-Binarizing:\", y_train[0])\n",
    "mlb = MultiLabelBinarizer(classes = range(0,15))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.fit_transform(y_test)\n",
    "print(\"Post-Binarizing:\", y_train[0])\n",
    "\n",
    "#Create the benchmark predictions by multiplying the half-time scores by 2\n",
    "#The benchmark prediction is that the same result of the first half will occur in the second half\n",
    "benchmark = [[2*i, 2*ii] for i, ii in half_time]\n",
    "benchmark = mlb.fit_transform(benchmark)\n",
    "print(len(benchmark))\n",
    "\n",
    "#Create OneVsRest SGDClassifier\n",
    "clf = OneVsRestClassifier(SGDClassifier(loss='modified_huber', penalty='elasticnet',\n",
    "                                          alpha=1e-4, n_iter=5, random_state=42,\n",
    "                                          shuffle=True, n_jobs=-1) )\n",
    "\n",
    "#Fit to our training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Saved with '2' after the name because my report and data uses the original and I do not want to overwrite it\n",
    "#as it makes understanding the report + results much easier\n",
    "with open('saved_SGD_&_train_test_data2.pkl', 'wb') as fid:\n",
    "    pickle.dump((clf, X_train, X_test, y_train, y_test, benchmark), fid,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SGDClassifier and compare to Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the total goals in the match described by the outputted vector\n",
    "def total_goals(goals):\n",
    "\n",
    "    total_goals = sum([elem*i for i, elem in enumerate(goals)])\n",
    "    return total_goals\n",
    "\n",
    "#Calculates the score described by the outputted vector\n",
    "def score_func(pred, actual):\n",
    "    score = 0\n",
    "\n",
    "    #10 points for exact score\n",
    "    if(np.array_equal(pred, actual)):\n",
    "        #print(\"10 Points:\", pred, actual)\n",
    "        return 10\n",
    "\n",
    "    #2 points for 1 correct score\n",
    "    #Subtract 1 array from the other, as they are not identical\n",
    "    #the result is either 2, 3 or 4 nonzero elements, 2 or 3 indicates a matching index\n",
    "    if(np.count_nonzero(pred-actual) <= 3):\n",
    "        return 2\n",
    "\n",
    "    #5 points for correct total goals\n",
    "    if(total_goals(pred) == total_goals(actual)):\n",
    "        return 5\n",
    "\n",
    "    #print(score, \" Points:\", pred, actual)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1482 1482 1482 1482 3554\n",
      "----------------\n",
      "Total Score\n",
      "----------------\n",
      "SGD:   3044\n",
      "Benchmark:  4559\n",
      "----------------\n",
      "Stats: \n",
      "SGD:  Matches where 0 points awarded:  319\n",
      "Benchmark: Matches where 0 points awarded:  146 \n",
      "\n",
      "SGD:  Matches where 2 points awarded:  1027\n",
      "Benchmark: Matches where 2 points awarded:  1082 \n",
      "\n",
      "SGD:  Matches where 5 points awarded:  74\n",
      "Benchmark: Matches where 5 points awarded:  29 \n",
      "\n",
      "SGD:  Matches where 10 points awarded:  62\n",
      "Benchmark: Matches where 10 points awarded:  225 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load data from the original saved_SGD_&_train_test_data.pkl\n",
    "with open('saved_SGD_&_train_test_data2.pkl', 'rb') as fid:\n",
    "    clf, X_train, X_test, y_train, y_test, benchmark = pickle.load(fid)\n",
    "\n",
    "#Make Predictions on X_test\n",
    "pred = clf.predict(X_test)\n",
    "#Since the output is a binary vector if there is only one 1 in it that means both teams scored that amount\n",
    "#For example: 0100 means the game was 1-1, in this case we need to change the 1 to a 2 for our scoring method to be accurate\n",
    "pred = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in pred]\n",
    "benchmark = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in benchmark]\n",
    "y_test = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in y_test]\n",
    "\n",
    "print(len(pred), len(benchmark), len(y_test), len(X_test), len(X_train))\n",
    "\n",
    "#Initialize Scoring variables\n",
    "benchmark_score = 0\n",
    "score= 0\n",
    "score_dict = dict({0: 0, 2: 0, 5: 0, 10: 0})\n",
    "benchmark_dict = dict({0: 0, 2: 0, 5: 0, 10: 0})\n",
    "\n",
    "for i in range(len(y_test)): \n",
    "\n",
    "    #One flaw in this model is that in a very small group of scenarios it indicates\n",
    "    #three goal scoring values as 1. This is a way to fix it. \n",
    "    #In the future I would make improvement here as indicated in my report  \n",
    "    if(np.count_nonzero(pred[i]) not in [1,2]):\n",
    "        pred[i][np.random.choice(np.nonzero(pred[i])[0])] = 0\n",
    "\n",
    "    if(np.count_nonzero(pred[i]) not in [1,2]):\n",
    "        print(\"Pred:\", i, pred[i])\n",
    "\n",
    "    #Perform the Scoring\n",
    "    temp = score_func(pred[i], y_test[i])\n",
    "    score_dict[temp] +=1\n",
    "    score += temp\n",
    "\n",
    "    temp = score_func(benchmark[i], y_test[i])\n",
    "    benchmark_dict[temp] +=1\n",
    "    benchmark_score += temp\n",
    "\n",
    "\n",
    "print(\"----------------\")\n",
    "print(\"Total Score\")\n",
    "print(\"----------------\")\n",
    "print(\"SGD:  \", score)\n",
    "print(\"Benchmark: \", benchmark_score)\n",
    "print(\"----------------\")\n",
    "print(\"Stats: \")\n",
    "for k in score_dict.keys():\n",
    "    print(\"SGD:  Matches where\", k, \"points awarded: \", score_dict[k])\n",
    "    print(\"Benchmark: Matches where\", k, \"points awarded: \", benchmark_dict[k], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SGD was created and saved successfully. It seems like it really pales in comparison to the benchmark, as it was outperformed in all categories except the correct number of total goals metric (matches where 5 points were awarded). Overall it is 1515 points behind the benchmark or 66.76% of the benchmark score. Ideally the NN will assist in rectifying this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
