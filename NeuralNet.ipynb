{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data for Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous data and initialize training lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loads the 'fixed' results and match schedule information\n",
    "with open('fixed_match_dates_and_scores.pkl', 'rb') as f:\n",
    "    results, match_dates = pickle.load(f)\n",
    "\n",
    "resultsSGD = deepcopy(results)\n",
    "    \n",
    "#Read in halftime match stats\n",
    "home_dfNN = pd.read_csv('NN/NN_home.csv', index_col=0)\n",
    "away_dfNN = pd.read_csv('NN/NN_away.csv', index_col=0)\n",
    "\n",
    "home_dfSGD = pd.read_csv('sgd_home.csv', index_col=0)\n",
    "away_dfSGD = pd.read_csv('sgd_away.csv', index_col=0)\n",
    "\n",
    "count, total = 0, 0\n",
    "X_train, X_test, y_train, y_test, X_trainSGD, X_testSGD, half_time = [], [], [], [], [], [], []\n",
    "\n",
    "#Set train/test split to 80/20\n",
    "test_train_limit = match_dates[0]+ int((match_dates[-1]-match_dates[0])*(4/5))\n",
    "\n",
    "#Loads the SGD Classifier and training/testing data created by SGDClassifier.ipynb\n",
    "with open('saved_SGD_&_train_test_data2.pkl', 'rb') as fid:\n",
    "    clf, _, _, _, _, _ = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Fixing Function for Preprocessing for Halftime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gives a score based on how much temp_name matches each name in namelist\n",
    "#The score is simply +1 for each consecutive matching letter\n",
    "#The longest length of consecutive matching letters is the score for each name in namelist\n",
    "def name_fix(temp_name, namelist):\n",
    "\n",
    "    #Special cases:\n",
    "    if(temp_name == 'EspanyolBarcelona'): return 'RCDEspanyol'\n",
    "    if(temp_name == 'QueensParkRangers'): return 'QPR'\n",
    "    if(temp_name == 'DeportivoLaCoruna'): return 'Dep.'\n",
    "    if(temp_name == 'LeicesterCity'): return 'Leicester'\n",
    "    if(temp_name == 'AtleticoMadrid'): return 'Atl.'\n",
    "    if(temp_name == 'BorussiaMonchengladbach'): return 'B.'\n",
    "   \n",
    "    \n",
    "    score = []\n",
    "    for elem in namelist:\n",
    "        count = 0\n",
    "        i = 0\n",
    "        curr_word = ''\n",
    "        letters = list(temp_name)\n",
    "        \n",
    "        #While the score is less than the length of the word in question\n",
    "        while i < len(letters):\n",
    "            #Increase the segment you are checking for matches by 1 letter\n",
    "            curr_word += letters[i]\n",
    "            \n",
    "            #If the new segment is in the name from namelist and it is longer than the\n",
    "            #max segment so far, store this as the maximum\n",
    "            if((curr_word in elem)): \n",
    "                if(count < len(curr_word)): \n",
    "                    count = len(curr_word)\n",
    "                    \n",
    "            #If the segment is not contained in the name, reset\n",
    "            elif(len(curr_word) != 1): \n",
    "                curr_word = ''\n",
    "                i-=1\n",
    "            \n",
    "            i+=1    \n",
    "\n",
    "        score.append(count)\n",
    "        \n",
    "    #The name with the highest score is returned    \n",
    "    return namelist[np.argmax(score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Name Fixing function above to clean up the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bastia', 'Marseille', (1, 2), (3, 3)], ['EvianThononGaillard', 'Caen', (0, 3), (0, 3)], ['Guingamp', 'SaintEtienne', (0, 1), (0, 2)], ['Lille', 'Metz', (0, 0), (0, 0)], ['Montpellier', 'Bordeaux', (0, 1), (0, 1)], ['Nantes', 'Lens', (0, 0), (1, 0)], ['Nice', 'Toulouse', (1, 2), (3, 2)]]\n",
      "[['Bastia', 'Marseille', (1, 2), (3, 3)], ['EvianTG', 'Caen', (0, 3), (0, 3)], ['Guingamp', 'StEtienne', (0, 1), (0, 2)], ['Lille', 'Metz', (0, 0), (0, 0)], ['Montpellier', 'Bordeaux', (0, 1), (0, 1)], ['Nantes', 'Lens', (0, 0), (1, 0)], ['Nice', 'Toulouse', (1, 2), (3, 2)]]\n"
     ]
    }
   ],
   "source": [
    "#Get list of team names according to the fulltime data\n",
    "match_id_list_home = list(home_dfNN.index)\n",
    "match_id_list_away = list(away_dfNN.index)\n",
    "match_id_list_home = list(set([elem.split('_')[0] for elem in match_id_list_home]))\n",
    "match_id_list_away = list(set([elem.split('_')[0] for elem in match_id_list_away]))\n",
    "\n",
    "\n",
    "#The current list is fixed in relation to the fulltime data\n",
    "#We need to 'fix' it in relation to the NN data\n",
    "#At the end of this we will have data that does not need to be fixed any more so in the future\n",
    "#but for the sake of submission I've left in this step (which doesn't do any harm)\n",
    "print(results[5331])\n",
    "\n",
    "#For each date where a game was played\n",
    "for date in match_dates:\n",
    "    #For each match played that day\n",
    "    for match in results[date]:\n",
    "            #Replace the name in the schedule with the equivalent name in the fulltime data\n",
    "            match[0] = name_fix(match[0], match_id_list_home)\n",
    "            match[1] = name_fix(match[1], match_id_list_away)\n",
    "\n",
    "#Save our fixed data\n",
    "with open('final_fixed_match_dates_and_scores.pkl', 'wb') as fid:\n",
    "    pickle.dump((results, match_dates), fid)    \n",
    "\n",
    "#Fixed names to verify visually\n",
    "print(results[5331])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in from the Dataframes, fill Training and Testing variables with the appropriate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: 3554 1479 3554 1479\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "#For each date where a game was played\n",
    "for date in match_dates:\n",
    "    #For each match played that day\n",
    "    for matchNN, matchSGD in zip(results[date], resultsSGD[date]):\n",
    "        \n",
    "        #Initialize for each match\n",
    "        temp_vec = 0\n",
    "        flag = [False, False]\n",
    "        home_nameNN = matchNN[0]\n",
    "        away_nameNN = matchNN[1]\n",
    "        \n",
    "        home_nameSGD = matchSGD[0]\n",
    "        away_nameSGD = matchSGD[1]\n",
    "\n",
    "\n",
    "        #Only look for data from prior to the match date for the Home team\n",
    "        #This way we do not use data from the match in our prediction of the match\n",
    "        temp_date = date - 1\n",
    "\n",
    "        #Iterate through the previous match days until you find a prior set of fulltime data\n",
    "        #In this dataframe the fulltime data is a rolling average of the previous fulltime data in the same season\n",
    "        while(temp_date > match_dates[0]):\n",
    "            #if there is data for the home team on temp_date day then load it\n",
    "            try:\n",
    "                temp_vec = [[float(elem) for elem in (home_dfSGD.loc[home_nameSGD+'_'+str(temp_date), :]).as_matrix()]]\n",
    "                \n",
    "                #When we find the data, indicate that we have found it and exit this loop\n",
    "                flag[0] = True\n",
    "                break\n",
    "            #if there is no data, check the day before\n",
    "            except:\n",
    "                temp_date -=1\n",
    "                \n",
    "\n",
    "        #Reset and Reuse\n",
    "        temp_date = date -1\n",
    "\n",
    "        #Identical to above but for the Away Team\n",
    "        while(temp_date > match_dates[0] and flag[0] == True): \n",
    "            try:    \n",
    "                temp_vec.append([float(elem) for elem in away_dfSGD.loc[away_nameSGD+'_'+str(temp_date), :].as_matrix()])\n",
    "                flag[1] = True\n",
    "                break\n",
    "            except:\n",
    "                temp_date -=1\n",
    "                \n",
    "        #If we have found prior data for both teams\n",
    "        if(flag[0] and flag[1]):\n",
    "            #Since match data is chronologically ordered, the earlier matches will be placed in \n",
    "            #the train category and the later matches in the test category\n",
    "            try:\n",
    "                #Since we found historical fulltime data for the match\n",
    "                #let us look for halftime data for the match (almost all, but not all matches had halftime data recorded)\n",
    "                vec = [[float(0) if elem == 'N' or elem == 'n' else float(elem) for elem in (home_dfNN.loc[home_nameNN+'_'+str(date), :]).as_matrix()]]\n",
    "                vec.append([float(0) if elem == 'N' or elem == 'n' else float(elem) for elem in away_dfNN.loc[away_nameNN+'_'+str(date), :].as_matrix()])\n",
    "                halftime_score = [float(elem) for elem in matchNN[2]]\n",
    "                vec[0].append(halftime_score[0])\n",
    "                vec[1].append(halftime_score[1])\n",
    "            \n",
    "                #If lower than the train/test limit add it to the training data\n",
    "                #otherwise add it to the testing data\n",
    "                if(date < test_train_limit):\n",
    "                    X_train.append(vec)\n",
    "                    X_trainSGD.append(temp_vec)\n",
    "                    y_train.append(list(matchNN[3]))\n",
    "                else:\n",
    "                    X_testSGD.append(temp_vec)\n",
    "                    y_test.append(list(matchNN[3]))\n",
    "                    X_test.append(vec)\n",
    "                    \n",
    "                    #Beginning creation of the Benchmark\n",
    "                    half_time.append(list(matchNN[2]))\n",
    "            except:\n",
    "                #A small number of matches did not have half time data we must ignore the SGD data for these\n",
    "                pass\n",
    "                \n",
    "\n",
    "print(\"Data Shape:\", len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format all Data for use in the NN and SGD, also create Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 2, 11)\n",
      "Pre-Shape Xtrain: [[ 40.   1.   3.   0.   6.   0.   0.  13.   6.   6.   0.]\n",
      " [ 60.   6.   6.   0.   4.   3.   0.  21.   1.   6.   0.]]\n",
      "(3554, 22)\n",
      "Re-Shape Xtrain: [ 40.   1.   3.   0.   6.   0.   0.  13.   6.   6.   0.  60.   6.   6.   0.\n",
      "   4.   3.   0.  21.   1.   6.   0.]\n",
      "Pre-Binarizing: [1 2]\n",
      "Post-Binarizing: [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Convert the data into numpy arrays\n",
    "X_train, X_test, X_trainSGD, X_testSGD = np.array(X_train), np.array(X_test), np.array(X_trainSGD), np.array(X_testSGD)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "half_time = np.array(half_time)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Pre-Shape Xtrain:\", X_train[0])\n",
    "\n",
    "#Classifier Replace all NaN Values with 0\n",
    "#This makes sense because some values (eg: red cards or yellow cards) when scraped were represented with \n",
    "#empty space rather than a 0\n",
    "X_train = np.nan_to_num(X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.nan_to_num(X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "X_trainSGD = np.nan_to_num(X_trainSGD.reshape(X_trainSGD.shape[0], X_trainSGD.shape[1]*X_trainSGD.shape[2]))\n",
    "X_testSGD = np.nan_to_num(X_testSGD.reshape(X_testSGD.shape[0], X_testSGD.shape[1]*X_testSGD.shape[2]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Re-Shape Xtrain:\", X_train[0])\n",
    "\n",
    "\n",
    "#MultilabelBinarizer to convert scores into a binary vector\n",
    "print(\"Pre-Binarizing:\", y_train[0])\n",
    "mlb = MultiLabelBinarizer(classes = range(0,15))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.fit_transform(y_test)\n",
    "print(\"Post-Binarizing:\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Fit [[4, 0], [6, 2]]\n",
      "Post-fit Benchmark: [[1 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Create the benchmark predictions by multiplying the half-time scores by 2\n",
    "#The benchmark prediction is that the same result of the first half will occur in the second half\n",
    "benchmark = [[2*i, 2*ii] for i, ii in half_time]\n",
    "print(\"Pre-Fit\", benchmark[0:2])\n",
    "benchmark = mlb.fit_transform(benchmark)\n",
    "\n",
    "print(\"Post-fit Benchmark:\", benchmark[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the total goals in the match described by the outputted vector\n",
    "def total_goals(goals):\n",
    "\n",
    "    total_goals = sum([elem*i for i, elem in enumerate(goals)])\n",
    "    return total_goals\n",
    "\n",
    "#Calculates the score described by the outputted vector\n",
    "def score_func(pred, actual):\n",
    "    score = 0\n",
    "    \n",
    "    #10 points for exact score\n",
    "    if(np.array_equal(pred, actual)):\n",
    "        #print(\"10 Points:\", pred, actual)\n",
    "        return 10\n",
    "\n",
    "    #2 points for 1 correct score\n",
    "    #Subtract 1 array from the other, as they are not identical\n",
    "    #the result is either 2, 3 or 4 nonzero elements, 2 or 3 indicates that 1 index matched\n",
    "    if(np.count_nonzero(pred-actual) <= 3):\n",
    "        return 2\n",
    "\n",
    "    #5 points for correct total goals\n",
    "    if(total_goals(pred) == total_goals(actual)):\n",
    "        return 5\n",
    "\n",
    "    #print(score, \" Points:\", pred, actual)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Display the Formatted Training/Testing data and Format the SGD training/testing data for last check before running the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3554, 15) (1479, 15)\n",
      "[ 40.   1.   3.   0.   6.   0.   0.  13.   6.   6.   0.  60.   6.   6.   0.\n",
      "   4.   3.   0.  21.   1.   6.   0.]\n",
      "[ 70.   3.   4.   2.   8.   5.   0.   0.   0.   2.   2.  30.   0.   1.   1.\n",
      "   2.   2.   1.   0.   1.   7.   0.]\n",
      "[0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Make predictions using the SGD to feed into the NN\n",
    "X_trainSGD = np.array(X_trainSGD)\n",
    "X_testSGD = np.array(X_testSGD)\n",
    "SGD_train = clf.predict(X_trainSGD)         \n",
    "SGD_test = clf.predict(X_testSGD)\n",
    "\n",
    "#Display a sample of each of these datas just to visually check everything \n",
    "#is going smoothly one last time\n",
    "print(SGD_train.shape, SGD_test.shape)\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "print(y_train[0])\n",
    "print(y_test[0])\n",
    "\n",
    "#Save the NN training/testing data\n",
    "with open('NN_Data_Struct.pkl', 'wb') as fid:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), fid) \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Training/Testing data ready for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reload the complete training/testing data\n",
    "with open('NN_Data_Struct.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    " \n",
    "\n",
    "#Add the predicted ouptut from the SGDClassifier to the \n",
    "#Training and testing feature vectors for the Neural Network\n",
    "X_trainf = [np.append(i, ii) for i, ii in zip(X_train, SGD_train)]\n",
    "X_testf = [np.append(i, ii) for i, ii in zip(X_test, SGD_test)]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_trainf)\n",
    "X_train = scaler.transform(X_trainf)\n",
    "X_test = scaler.transform(X_testf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all Nerual Network Parameters and Create the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Set the number of nodes at each layer\n",
    "nodes1 = 1024\n",
    "nodes2 = 512\n",
    "input_nodes = X_train.shape[1]\n",
    "output_nodes = y_train.shape[1]\n",
    "X = tf.placeholder(\"float\", [None, input_nodes])\n",
    "Y = tf.placeholder(\"float\", [None, output_nodes])\n",
    "\n",
    "# Create weights and biases for each layer\n",
    "W1 = tf.Variable(tf.random_normal([input_nodes, nodes1]))\n",
    "W2 = tf.Variable(tf.random_normal([nodes1, nodes2]))\n",
    "W3 = tf.Variable(tf.random_normal([nodes2, output_nodes]))\n",
    "B1 = tf.Variable(tf.random_normal([nodes1]))\n",
    "B2 = tf.Variable(tf.random_normal([nodes2]))\n",
    "B3 = tf.Variable(tf.random_normal([output_nodes]))\n",
    "\n",
    "#Create 2 hidden layers and the output layer\n",
    "def propogate(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, W1), B1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W2), B2)\n",
    "    output_layer = tf.matmul(layer_2, W3) + B3\n",
    "    return output_layer\n",
    "\n",
    "logits = propogate(X)\n",
    "\n",
    "#Compute softmax cross entropy between logits and labels\n",
    "cross_entropy= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "#Adagrad for gradient-based optimization\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.0001)\n",
    "\n",
    "#Compute the gradients and apply them to the variables\n",
    "optimize = optimizer.minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Net and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Cost: 2041.90412176\n",
      "Epoch: 6 | Cost: 915.28150368\n",
      "Epoch: 11 | Cost: 915.303378662\n",
      "Epoch: 16 | Cost: 923.742325866\n",
      "Epoch: 21 | Cost: 935.812523709\n",
      "Epoch: 26 | Cost: 949.952368413\n",
      "Epoch: 31 | Cost: 965.449421472\n",
      "Epoch: 36 | Cost: 981.408413298\n",
      "----------------\n",
      "Total Score\n",
      "----------------\n",
      "NN & SGD:   3708\n",
      "Benchmark:  4553\n",
      "----------------\n",
      "Stats: \n",
      "NN & SGD:  Matches where 0 points awarded:  73\n",
      "Benchmark: Matches where 0 points awarded:  146 \n",
      "\n",
      "NN & SGD:  Matches where 2 points awarded:  1289\n",
      "Benchmark: Matches where 2 points awarded:  1079 \n",
      "\n",
      "NN & SGD:  Matches where 5 points awarded:  8\n",
      "Benchmark: Matches where 5 points awarded:  29 \n",
      "\n",
      "NN & SGD:  Matches where 10 points awarded:  109\n",
      "Benchmark: Matches where 10 points awarded:  225 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #For 45 epochs\n",
    "    for epoch in range(40):\n",
    "        avgCost = 0\n",
    "        \n",
    "        #For each feature/label combo in the training set\n",
    "        for i in range(len(X_train)):\n",
    "           \n",
    "            #Set the current feature/label combo\n",
    "            x, y = X_train[i: i + 1], y_train[i: i + 1]\n",
    "            \n",
    "            #Call the Loss function and AdagradOptimizer for it\n",
    "            _, cost = sess.run([optimize, cross_entropy], feed_dict={X: x, Y: y})\n",
    "            \n",
    "            #Calculate average cost so we can monitor what the NN is up to\n",
    "            avgCost+= cost / len(X_train)\n",
    "            \n",
    "        if(epoch%5 == 0):\n",
    "            print(\"Epoch:\", (epoch+1), \"| Cost:\", avgCost)\n",
    "\n",
    "    #Find the predicted value for all X_test values by calling sess.run on the softmax of output node\n",
    "    output = tf.nn.softmax(logits)\n",
    "    pred = sess.run(output, feed_dict={X: X_test})\n",
    "    \n",
    "    #If there is only one 1 in the vector we should double it inorder to indicate both teams scored that amount\n",
    "    benchmark = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in benchmark]\n",
    "    y_test = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in y_test]\n",
    "    \n",
    "    \n",
    "    points, temp = [0,0]\n",
    "    score_dict = dict({0: 0, 2: 0, 5: 0, 10: 0})\n",
    "    \n",
    "    \n",
    "    #Calculate the score for the NN & SGD model\n",
    "    for i in range(len(pred)):\n",
    "        p = pred[i]\n",
    "        t = y_test[i]     \n",
    "        s = SGD_test[i]\n",
    "\n",
    "        #One last way of incorporating the SGD predictions was to\n",
    "        #average them into the neural network output and then round off\n",
    "        #the answer to the nearest whole number\n",
    "        #The import thing is not the actual number but the magnitude of it in comparison the rest\n",
    "        final = np.mean(np.array([p, s]), axis = 0)\n",
    "        final = [round(elem) for elem in final]\n",
    "        \n",
    "        #If we have 3+ nonzero numbers, we need to get down to 1 or 2\n",
    "        #This problem was discussed in the SGD and seems to be a byproduct of using\n",
    "        #the average.\n",
    "        while(np.count_nonzero(final) not in [0, 1, 2]):\n",
    "             final[np.random.choice(np.nonzero(final)[0])] = 0\n",
    "        \n",
    "        #If any of the newly created vectors is all zeroes just use the original\n",
    "        #Neural network output instead\n",
    "        for ind in range(len(final)):\n",
    "            if(np.count_nonzero(final[ind]) == 0):\n",
    "                final = p\n",
    "                \n",
    "        #Only 1 index means both teams scored that amount so we must double it\n",
    "        final = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in final]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Perform the Scoring\n",
    "        temp = score_func(final, t)\n",
    "        score_dict[temp] +=1\n",
    "        points += temp\n",
    "    \n",
    "   \n",
    "    points2, temp2 = [0,0]\n",
    "    score_dict2 = dict({0: 0, 2: 0, 5: 0, 10: 0})\n",
    "    \n",
    "    #Calculate the score for the benchmark model\n",
    "    for t, p in zip(y_test, benchmark):\n",
    "           \n",
    "        #Perform the Scoring\n",
    "        temp2 = score_func(p, t)\n",
    "        score_dict2[temp2] +=1\n",
    "        points2 += temp2\n",
    "\n",
    "    \n",
    "    #Display Results\n",
    "    print(\"----------------\")\n",
    "    print(\"Total Score\")\n",
    "    print(\"----------------\")\n",
    "    print(\"NN & SGD:  \", points)\n",
    "    print(\"Benchmark: \", points2)\n",
    "    print(\"----------------\")\n",
    "    print(\"Stats: \")\n",
    "    for k in score_dict.keys():\n",
    "        print(\"NN & SGD:  Matches where\", k, \"points awarded: \", score_dict[k])\n",
    "        print(\"Benchmark: Matches where\", k, \"points awarded: \", score_dict2[k], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The combined NN & SGD model was still unable to surpass the benchmark model. It did however make a huge improvement compared to only the SGDClassifier. The original difference between the benchmark and SGDClassifier was 1515 points, now with the NN & SGD model it is only 765. Described differently the SGDClassifier model had 66.76% of the points of the benchmark, while the final model had 83.20% of the benchmark. The benchmark scores are slightly different between the two comparisons because there were fewer points avaiable in this most recent test as some halftime match data was not available. \n",
    "\n",
    "The only category where the NN & SGD model was able to surpass the benchmark was the 2 point category (where only 1 score matched the true result). Unfortunately, the most important categories for predicting the number of goals or the precise score correctly were dominated by the benchmark model. I tried a variety of methods to improve this (detailed in the report), however the main conclusion I reached is that for this problem, doubling the halftime score is an incredibly good benchmark. It was able to predict the precise correct fulltime score in 15.21% of matches. That's an incredibly high rate for such a simple statistic! In fact, 49.42% of the total points collected by the benchmark were earned that way. In comparison, only 29.30% of the NN & SGD model's points were collected through a precise fulltime score prediction. Maybe I will use the doubled halftimescore as part of the model the next time I approach this problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
