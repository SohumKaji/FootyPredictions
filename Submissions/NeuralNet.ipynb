{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Net Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data for Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Previous data and initialize training lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loads the 'fixed' results and match schedule information\n",
    "with open('fixed_match_dates_and_scores.pkl', 'rb') as f:\n",
    "    results, match_dates = pickle.load(f)\n",
    "\n",
    "resultsSGD = deepcopy(results)\n",
    "    \n",
    "#Read in halftime match stats\n",
    "home_dfNN = pd.read_csv('NN/NN_home.csv', index_col=0)\n",
    "away_dfNN = pd.read_csv('NN/NN_away.csv', index_col=0)\n",
    "\n",
    "home_dfSGD = pd.read_csv('sgd_home.csv', index_col=0)\n",
    "away_dfSGD = pd.read_csv('sgd_away.csv', index_col=0)\n",
    "\n",
    "count, total = 0, 0\n",
    "X_train, X_test, y_train, y_test, X_trainSGD, X_testSGD, half_time = [], [], [], [], [], [], []\n",
    "\n",
    "#Set train/test split to 80/20\n",
    "test_train_limit = match_dates[0]+ int((match_dates[-1]-match_dates[0])*(4/5))\n",
    "\n",
    "#Loads the SGD Classifier and training/testing data created by SGDClassifier.ipynb\n",
    "with open('saved_SGD_&_train_test_data2.pkl', 'rb') as fid:\n",
    "    clf, _, _, _, _, _ = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Fixing Function for Preprocessing for Halftime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gives a score based on how much temp_name matches each name in namelist\n",
    "#The score is simply +1 for each consecutive matching letter\n",
    "#The longest length of consecutive matching letters is the score for each name in namelist\n",
    "def name_fix(temp_name, namelist):\n",
    "\n",
    "    #Special cases:\n",
    "    if(temp_name == 'EspanyolBarcelona'): return 'RCDEspanyol'\n",
    "    if(temp_name == 'QueensParkRangers'): return 'QPR'\n",
    "    if(temp_name == 'DeportivoLaCoruna'): return 'Dep.'\n",
    "    if(temp_name == 'LeicesterCity'): return 'Leicester'\n",
    "    if(temp_name == 'AtleticoMadrid'): return 'Atl.'\n",
    "    if(temp_name == 'BorussiaMonchengladbach'): return 'B.'\n",
    "   \n",
    "    \n",
    "    score = []\n",
    "    for elem in namelist:\n",
    "        count = 0\n",
    "        i = 0\n",
    "        curr_word = ''\n",
    "        letters = list(temp_name)\n",
    "        \n",
    "        #While the score is less than the length of the word in question\n",
    "        while i < len(letters):\n",
    "            #Increase the segment you are checking for matches by 1 letter\n",
    "            curr_word += letters[i]\n",
    "            \n",
    "            #If the new segment is in the name from namelist and it is longer than the\n",
    "            #max segment so far, store this as the maximum\n",
    "            if((curr_word in elem)): \n",
    "                if(count < len(curr_word)): \n",
    "                    count = len(curr_word)\n",
    "                    \n",
    "            #If the segment is not contained in the name, reset\n",
    "            elif(len(curr_word) != 1): \n",
    "                curr_word = ''\n",
    "                i-=1\n",
    "            \n",
    "            i+=1    \n",
    "\n",
    "        score.append(count)\n",
    "        \n",
    "    #The name with the highest score is returned    \n",
    "    return namelist[np.argmax(score)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Name Fixing function above to clean up the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Bastia', 'Marseille', (1, 2), (3, 3)], ['EvianThononGaillard', 'Caen', (0, 3), (0, 3)], ['Guingamp', 'SaintEtienne', (0, 1), (0, 2)], ['Lille', 'Metz', (0, 0), (0, 0)], ['Montpellier', 'Bordeaux', (0, 1), (0, 1)], ['Nantes', 'Lens', (0, 0), (1, 0)], ['Nice', 'Toulouse', (1, 2), (3, 2)]]\n",
      "[['Bastia', 'Marseille', (1, 2), (3, 3)], ['EvianTG', 'Caen', (0, 3), (0, 3)], ['Guingamp', 'StEtienne', (0, 1), (0, 2)], ['Lille', 'Metz', (0, 0), (0, 0)], ['Montpellier', 'Bordeaux', (0, 1), (0, 1)], ['Nantes', 'Lens', (0, 0), (1, 0)], ['Nice', 'Toulouse', (1, 2), (3, 2)]]\n"
     ]
    }
   ],
   "source": [
    "#Get list of team names according to the fulltime data\n",
    "match_id_list_home = list(home_dfNN.index)\n",
    "match_id_list_away = list(away_dfNN.index)\n",
    "match_id_list_home = list(set([elem.split('_')[0] for elem in match_id_list_home]))\n",
    "match_id_list_away = list(set([elem.split('_')[0] for elem in match_id_list_away]))\n",
    "\n",
    "\n",
    "#The current list is fixed in relation to the fulltime data\n",
    "#We need to 'fix' it in relation to the NN data\n",
    "#At the end of this we will have data that does not need to be fixed any more so in the future\n",
    "#but for the sake of submission I've left in this step (which doesn't do any harm)\n",
    "print(results[5331])\n",
    "\n",
    "#For each date where a game was played\n",
    "for date in match_dates:\n",
    "    #For each match played that day\n",
    "    for match in results[date]:\n",
    "            #Replace the name in the schedule with the equivalent name in the fulltime data\n",
    "            match[0] = name_fix(match[0], match_id_list_home)\n",
    "            match[1] = name_fix(match[1], match_id_list_away)\n",
    "\n",
    "#Save our fixed data\n",
    "with open('final_fixed_match_dates_and_scores.pkl', 'wb') as fid:\n",
    "    pickle.dump((results, match_dates), fid)    \n",
    "\n",
    "#Fixed names to verify visually\n",
    "print(results[5331])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in from the Dataframes, fill Training and Testing variables with the appropriate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parma_5415 ACMilan_5415\n",
      "ACMilan_5423 Verona_5423\n",
      "ACMilan_5437 ACMilan_5437\n",
      "ASRoma_5444 ACMilan_5444\n",
      "ACMilan_5451 Udinese_5451\n",
      "Chievo_5459 ACMilan_5459\n",
      "ACMilan_5465 Lazio_5465\n",
      "ACMilan_5486 Genoa_5486\n",
      "Empoli_5492 ACMilan_5492\n",
      "Sassuolo_5507 ACMilan_5507\n",
      "ACMilan_5514 Palermo_5514\n",
      "ACMilan_5535 Fiorentina_5535\n",
      "ACMilan_5549 Cesena_5549\n",
      "Sampdoria_5556 ACMilan_5556\n",
      "ACMilan_5569 Parma_5569\n",
      "Verona_5576 ACMilan_5576\n",
      "ACMilan_5584 ACMilan_5584\n",
      "Udinese_5593 ACMilan_5593\n",
      "ACMilan_5590 ASRoma_5590\n",
      "ACMilan_5598 Chievo_5598\n",
      "Lazio_5605 ACMilan_5605\n",
      "ACMilan_5611 Juventus_5611\n",
      "ACMilan_5500 Torino_5500\n",
      "Atalanta_5521 ACMilan_5521\n",
      "Cagliari_5529 ACMilan_5529\n",
      "Napoli_5542 ACMilan_5542\n",
      "Genoa_5618 ACMilan_5618\n",
      "ACMilan_5412 Sampdoria_5412\n",
      "Juventus_5481 ACMilan_5481\n",
      "ACMilan_5626 Empoli_5626\n",
      "ACMilan_5710 Atalanta_5710\n",
      "ACMilan_5731 ACMilan_5731\n",
      "Chievo_5738 ACMilan_5738\n",
      "ACMilan_5745 Fiorentina_5745\n",
      "Sampdoria_5752 ACMilan_5752\n",
      "ACMilan_5766 Juventus_5766\n",
      "Palermo_5772 ACMilan_5772\n",
      "ACMilan_5779 ASRoma_5779\n",
      "Torino_5787 ACMilan_5787\n",
      "ACMilan_5801 Frosinone_5801\n",
      "ACMilan_5814 Genoa_5814\n",
      "Udinese_5821 ACMilan_5821\n",
      "ACMilan_5829 Lazio_5829\n",
      "Atalanta_5856 ACMilan_5856\n",
      "ACMilan_5864 Carpi_5864\n",
      "ACMilan_5874 Chievo_5874\n",
      "Verona_5878 ACMilan_5878\n",
      "Fiorentina_5885 ACMilan_5885\n",
      "Juventus_5899 ACMilan_5899\n",
      "Genoa_5951 ACMilan_5951\n",
      "ACMilan_5906 Palermo_5906\n",
      "ACMilan_5912 Bologna_5912\n",
      "ASRoma_5919 ACMilan_5919\n",
      "ACMilan_5934 Torino_5934\n",
      "Frosinone_5940 ACMilan_5940\n",
      "ACMilan_5947 Napoli_5947\n",
      "ACMilan_5954 Udinese_5954\n",
      "Lazio_5962 ACMilan_5962\n",
      "ACMilan_5968 Empoli_5968\n",
      "ACMilan_5741 Verona_5741\n",
      "ACMilan_5850 Sassuolo_5850\n",
      "ACMilan_5871 ACMilan_5871\n",
      "ACMilan_5891 Sampdoria_5891\n",
      "Sassuolo_5975 ACMilan_5975\n",
      "Bologna_5775 ACMilan_5775\n",
      "Napoli_5809 ACMilan_5809\n",
      "Empoli_5846 ACMilan_5846\n",
      "Chievo_6074 ACMilan_6074\n",
      "ACMilan_6081 Palermo_6081\n",
      "Pescara_6095 ACMilan_6095\n",
      "ACMilan_6102 Juventus_6102\n",
      "ACMilan_6109 Bologna_6109\n",
      "ASRoma_6116 ACMilan_6116\n",
      "ACMilan_6130 Cagliari_6130\n",
      "Atalanta_6137 ACMilan_6137\n",
      "Sampdoria_6144 ACMilan_6144\n",
      "ACMilan_6151 Crotone_6151\n",
      "ACMilan_6165 ACMilan_6165\n",
      "ACMilan_6186 Genoa_6186\n",
      "Sassuolo_6193 ACMilan_6193\n",
      "ACMilan_6219 Chievo_6219\n",
      "Palermo_6227 ACMilan_6227\n",
      "Juventus_6241 ACMilan_6241\n",
      "ACMilan_6248 Empoli_6248\n",
      "ACMilan_6262 ASRoma_6262\n",
      "Cagliari_6269 ACMilan_6269\n",
      "ACMilan_6276 Atalanta_6276\n",
      "Torino_6282 ACMilan_6282\n",
      "Villarreal_6296 Eibar_6296\n",
      "Crotone_6304 ACMilan_6304\n",
      "ACMilan_6310 ACMilan_6310\n",
      "Fiorentina_6317 ACMilan_6317\n",
      "ACMilan_6325 Napoli_6325\n",
      "Genoa_6332 ACMilan_6332\n",
      "ACMilan_6339 Sassuolo_6339\n",
      "Lazio_6346 ACMilan_6346\n",
      "Empoli_6105 ACMilan_6105\n",
      "ACMilan_6173 Fiorentina_6173\n",
      "Udinese_6213 ACMilan_6213\n",
      "ACMilan_6233 Pescara_6233\n",
      "Bologna_6255 ACMilan_6255\n",
      "Espanyol_6295 Betis_6295\n",
      "HerthaBerlin_6295 Hoffenheim_6295\n",
      "ACMilan_6298 Sampdoria_6298\n",
      "Napoli_6177 ACMilan_6177\n",
      "ACMilan_6196 Lazio_6196\n",
      "ACMilan_6140 Torino_6140\n",
      "ACMilan_6353 Udinese_6353\n",
      "Missing Halftime data for 108 matches\n",
      "Data Shape: 3480 1448 3480 1448\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "#For each date where a game was played\n",
    "for date in match_dates:\n",
    "    #For each match played that day\n",
    "    for matchNN, matchSGD in zip(results[date], resultsSGD[date]):\n",
    "        \n",
    "        #Initialize for each match\n",
    "        temp_vec = 0\n",
    "        flag = [False, False]\n",
    "        home_nameNN = matchNN[0]\n",
    "        away_nameNN = matchNN[1]\n",
    "        \n",
    "        home_nameSGD = matchSGD[0]\n",
    "        away_nameSGD = matchSGD[1]\n",
    "\n",
    "\n",
    "        #Only look for data from prior to the match date for the Home team\n",
    "        #This way we do not use data from the match in our prediction of the match\n",
    "        temp_date = date - 1\n",
    "\n",
    "        #Iterate through the previous match days until you find a prior set of fulltime data\n",
    "        #In this dataframe the fulltime data is a rolling average of the previous fulltime data in the same season\n",
    "        while(temp_date > match_dates[0]):\n",
    "            #if there is data for the home team on temp_date day then load it\n",
    "            try:\n",
    "                temp_vec = [[float(elem) for elem in (home_dfSGD.loc[home_nameSGD+'_'+str(temp_date), :]).as_matrix()]]\n",
    "                \n",
    "                #When we find the data, indicate that we have found it and exit this loop\n",
    "                flag[0] = True\n",
    "                break\n",
    "            #if there is no data, check the day before\n",
    "            except:\n",
    "                temp_date -=1\n",
    "                \n",
    "\n",
    "        #Reset and Reuse\n",
    "        temp_date = date -1\n",
    "\n",
    "        #Identical to above but for the Away Team\n",
    "        while(temp_date > match_dates[0] and flag[0] == True): \n",
    "            try:    \n",
    "                temp_vec.append([float(elem) for elem in away_dfSGD.loc[away_nameSGD+'_'+str(temp_date), :].as_matrix()])\n",
    "                flag[1] = True\n",
    "                break\n",
    "            except:\n",
    "                temp_date -=1\n",
    "                \n",
    "        #If we have found prior data for both teams\n",
    "        if(flag[0] and flag[1]):\n",
    "            #Since match data is chronologically ordered, the earlier matches will be placed in \n",
    "            #the train category and the later matches in the test category\n",
    "            try:\n",
    "                #Since we found historical fulltime data for the match\n",
    "                #let us look for halftime data for the match (almost all, but not all matches had halftime data recorded)\n",
    "                vec = [[float(0) if elem == 'N' or elem == 'n' else float(elem) for elem in (home_dfNN.loc[home_nameNN+'_'+str(date), :]).as_matrix()]]\n",
    "                vec.append([float(0) if elem == 'N' or elem == 'n' else float(elem) for elem in away_dfNN.loc[away_nameNN+'_'+str(date), :].as_matrix()])\n",
    "                halftime_score = [float(elem) for elem in matchNN[2]]\n",
    "                vec[0].append(halftime_score[0])\n",
    "                vec[1].append(halftime_score[1])\n",
    "            \n",
    "                #If lower than the train/test limit add it to the training data\n",
    "                #otherwise add it to the testing data\n",
    "                if(date < test_train_limit):\n",
    "                    X_train.append(vec)\n",
    "                    X_trainSGD.append(temp_vec)\n",
    "                    y_train.append(list(matchNN[3]))\n",
    "                else:\n",
    "                    X_testSGD.append(temp_vec)\n",
    "                    y_test.append(list(matchNN[3]))\n",
    "                    X_test.append(vec)\n",
    "                    \n",
    "                    #Beginning creation of the Benchmark\n",
    "                    half_time.append(list(matchNN[2]))\n",
    "            except:\n",
    "                print(home_nameNN+'_'+str(date), away_nameNN+'_'+str(date))\n",
    "                total +=1\n",
    "                \n",
    "#A small number of matches did not have half time data. These matches are listed below.\n",
    "print(\"Missing Halftime data for\", total, \"matches\")\n",
    "print(\"Data Shape:\", len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format all Data for use in the NN and SGD, also create Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 2, 11)\n",
      "Pre-Shape Xtrain: [[ 40.   1.   3.   0.   6.   0.   0.  13.   6.   6.   0.]\n",
      " [ 60.   6.   6.   0.   4.   3.   0.  21.   1.   6.   0.]]\n",
      "(3480, 22)\n",
      "Re-Shape Xtrain: [ 40.   1.   3.   0.   6.   0.   0.  13.   6.   6.   0.  60.   6.   6.   0.\n",
      "   4.   3.   0.  21.   1.   6.   0.]\n",
      "Pre-Binarizing: [1 2]\n",
      "Post-Binarizing: [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Convert the data into numpy arrays\n",
    "X_train, X_test, X_trainSGD, X_testSGD = np.array(X_train), np.array(X_test), np.array(X_trainSGD), np.array(X_testSGD)\n",
    "y_train, y_test = np.array(y_train), np.array(y_test)\n",
    "half_time = np.array(half_time)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Pre-Shape Xtrain:\", X_train[0])\n",
    "\n",
    "#Classifier Replace all NaN Values with 0\n",
    "#This makes sense because some values (eg: red cards or yellow cards) when scraped were represented with \n",
    "#empty space rather than a 0\n",
    "X_train = np.nan_to_num(X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]))\n",
    "X_test = np.nan_to_num(X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]))\n",
    "X_trainSGD = np.nan_to_num(X_trainSGD.reshape(X_trainSGD.shape[0], X_trainSGD.shape[1]*X_trainSGD.shape[2]))\n",
    "X_testSGD = np.nan_to_num(X_testSGD.reshape(X_testSGD.shape[0], X_testSGD.shape[1]*X_testSGD.shape[2]))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(\"Re-Shape Xtrain:\", X_train[0])\n",
    "\n",
    "\n",
    "#MultilabelBinarizer to convert scores into a binary vector\n",
    "print(\"Pre-Binarizing:\", y_train[0])\n",
    "mlb = MultiLabelBinarizer(classes = range(0,15))\n",
    "y_train = mlb.fit_transform(y_train)\n",
    "y_test = mlb.fit_transform(y_test)\n",
    "print(\"Post-Binarizing:\", y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Fit [[4, 0], [6, 2]]\n",
      "Post-fit Benchmark: [[1 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#Create the benchmark predictions by multiplying the half-time scores by 2\n",
    "#The benchmark prediction is that the same result of the first half will occur in the second half\n",
    "benchmark = [[2*i, 2*ii] for i, ii in half_time]\n",
    "print(\"Pre-Fit\", benchmark[0:2])\n",
    "benchmark = mlb.fit_transform(benchmark)\n",
    "\n",
    "print(\"Post-fit Benchmark:\", benchmark[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculates the total goals in the match described by the outputted vector\n",
    "def total_goals(goals):\n",
    "\n",
    "    total_goals = sum([elem*i for i, elem in enumerate(goals)])\n",
    "    return total_goals\n",
    "\n",
    "#Calculates the score described by the outputted vector\n",
    "def score_func(pred, actual):\n",
    "    score = 0\n",
    "    \n",
    "    #10 points for exact score\n",
    "    if(np.array_equal(pred, actual)):\n",
    "        #print(\"10 Points:\", pred, actual)\n",
    "        return 10\n",
    "\n",
    "    #2 points for 1 correct score\n",
    "    #Subtract 1 array from the other, as they are not identical\n",
    "    #the result is either 2, 3 or 4 nonzero elements, 2 or 3 indicates that 1 index matched\n",
    "    if(np.count_nonzero(pred-actual) <= 3):\n",
    "        return 2\n",
    "\n",
    "    #5 points for correct total goals\n",
    "    if(total_goals(pred) == total_goals(actual)):\n",
    "        return 5\n",
    "\n",
    "    #print(score, \" Points:\", pred, actual)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and Display the Formatted Training/Testing data and Format the SGD training/testing data for last check before running the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3480, 15) (1448, 15)\n",
      "[ 40.   1.   3.   0.   6.   0.   0.  13.   6.   6.   0.  60.   6.   6.   0.\n",
      "   4.   3.   0.  21.   1.   6.   0.]\n",
      "[ 70.   3.   4.   2.   8.   5.   0.   0.   0.   2.   2.  30.   0.   1.   1.\n",
      "   2.   2.   1.   0.   1.   7.   0.]\n",
      "[0 1 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Make predictions using the SGD to feed into the NN\n",
    "X_trainSGD = np.array(X_trainSGD)\n",
    "X_testSGD = np.array(X_testSGD)\n",
    "SGD_train = clf.predict(X_trainSGD)         \n",
    "SGD_test = clf.predict(X_testSGD)\n",
    "\n",
    "#Display a sample of each of these datas just to visually check everything \n",
    "#is going smoothly one last time\n",
    "print(SGD_train.shape, SGD_test.shape)\n",
    "print(X_train[0])\n",
    "print(X_test[0])\n",
    "print(y_train[0])\n",
    "print(y_test[0])\n",
    "\n",
    "#Save the NN training/testing data\n",
    "with open('NN_Data_Struct.pkl', 'wb') as fid:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), fid) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Training/Testing data ready for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reload the complete training/testing data\n",
    "with open('NN_Data_Struct.pkl', 'rb') as f:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(f)\n",
    " \n",
    "\n",
    "#Add the predicted ouptut from the SGDClassifier to the \n",
    "#Training and testing feature vectors for the Neural Network\n",
    "X_trainf = [np.append(i, ii) for i, ii in zip(X_train, SGD_train)]\n",
    "X_testf = [np.append(i, ii) for i, ii in zip(X_test, SGD_test)]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_trainf)\n",
    "X_train = scaler.transform(X_trainf)\n",
    "X_test = scaler.transform(X_testf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set all Nerual Network Parameters and Create the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Set the number of nodes at each layer\n",
    "nodes1 = 1024\n",
    "nodes2 = 512\n",
    "input_nodes = X_train.shape[1]\n",
    "output_nodes = y_train.shape[1]\n",
    "X = tf.placeholder(\"float\", [None, input_nodes])\n",
    "Y = tf.placeholder(\"float\", [None, output_nodes])\n",
    "\n",
    "# Create weights and biases for each layer\n",
    "W1 = tf.Variable(tf.random_normal([input_nodes, nodes1]))\n",
    "W2 = tf.Variable(tf.random_normal([nodes1, nodes2]))\n",
    "W3 = tf.Variable(tf.random_normal([nodes2, output_nodes]))\n",
    "B1 = tf.Variable(tf.random_normal([nodes1]))\n",
    "B2 = tf.Variable(tf.random_normal([nodes2]))\n",
    "B3 = tf.Variable(tf.random_normal([output_nodes]))\n",
    "\n",
    "#Create 2 hidden layers and the output layer\n",
    "def propogate(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, W1), B1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W2), B2)\n",
    "    output_layer = tf.matmul(layer_2, W3) + B3\n",
    "    return output_layer\n",
    "\n",
    "logits = propogate(X)\n",
    "\n",
    "#Compute softmax cross entropy between logits and labels\n",
    "cross_entropy= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "\n",
    "#Adagrad for gradient-based optimization\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.0001)\n",
    "\n",
    "#Compute the gradients and apply them to the variables\n",
    "optimize = optimizer.minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural Net and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Cost: 1636.73171175\n",
      "Epoch: 2 | Cost: 1123.17022609\n",
      "Epoch: 3 | Cost: 1084.88874579\n",
      "Epoch: 4 | Cost: 1070.78461277\n",
      "Epoch: 5 | Cost: 1062.74074622\n",
      "Epoch: 6 | Cost: 1057.42403716\n",
      "Epoch: 7 | Cost: 1053.58618051\n",
      "Epoch: 8 | Cost: 1050.71861345\n",
      "Epoch: 9 | Cost: 1048.83935005\n",
      "Epoch: 10 | Cost: 1047.67709721\n",
      "Epoch: 11 | Cost: 1046.93288\n",
      "Epoch: 12 | Cost: 1046.58740744\n",
      "Epoch: 13 | Cost: 1046.6712303\n",
      "Epoch: 14 | Cost: 1047.04009024\n",
      "Epoch: 15 | Cost: 1047.6234768\n",
      "Epoch: 16 | Cost: 1048.22991215\n",
      "Epoch: 17 | Cost: 1048.92277569\n",
      "Epoch: 18 | Cost: 1049.65118354\n",
      "Epoch: 19 | Cost: 1050.51871088\n",
      "Epoch: 20 | Cost: 1051.59815769\n",
      "Epoch: 21 | Cost: 1052.873041\n",
      "Epoch: 22 | Cost: 1054.1458487\n",
      "Epoch: 23 | Cost: 1055.37768176\n",
      "Epoch: 24 | Cost: 1056.73049932\n",
      "Epoch: 25 | Cost: 1058.082202\n",
      "Epoch: 26 | Cost: 1059.46961873\n",
      "Epoch: 27 | Cost: 1060.95680835\n",
      "Epoch: 28 | Cost: 1062.5410683\n",
      "Epoch: 29 | Cost: 1064.14960839\n",
      "Epoch: 30 | Cost: 1065.7932029\n",
      "Epoch: 31 | Cost: 1067.49799814\n",
      "Epoch: 32 | Cost: 1069.19562879\n",
      "Epoch: 33 | Cost: 1070.92444475\n",
      "Epoch: 34 | Cost: 1072.72417281\n",
      "Epoch: 35 | Cost: 1074.56234954\n",
      "Epoch: 36 | Cost: 1076.43337867\n",
      "Epoch: 37 | Cost: 1078.31037674\n",
      "Epoch: 38 | Cost: 1080.2183904\n",
      "Epoch: 39 | Cost: 1082.11422991\n",
      "Epoch: 40 | Cost: 1084.09342947\n",
      "Epoch: 41 | Cost: 1086.11911872\n",
      "Epoch: 42 | Cost: 1088.16849302\n",
      "Epoch: 43 | Cost: 1090.21851159\n",
      "Epoch: 44 | Cost: 1092.32398396\n",
      "Epoch: 45 | Cost: 1094.43274871\n",
      "Epoch: 46 | Cost: 1096.54087405\n",
      "Epoch: 47 | Cost: 1098.67411529\n",
      "Epoch: 48 | Cost: 1100.88709369\n",
      "Epoch: 49 | Cost: 1103.14638141\n",
      "Epoch: 50 | Cost: 1105.40275051\n",
      "----------------\n",
      "Total Score\n",
      "----------------\n",
      "NN & SGD:   3687\n",
      "Benchmark:  4476\n",
      "----------------\n",
      "Stats: \n",
      "NN & SGD:  Matches where 0 points awarded:  45\n",
      "Benchmark: Matches where 0 points awarded:  140 \n",
      "\n",
      "NN & SGD:  Matches where 2 points awarded:  1291\n",
      "Benchmark: Matches where 2 points awarded:  1058 \n",
      "\n",
      "NN & SGD:  Matches where 5 points awarded:  3\n",
      "Benchmark: Matches where 5 points awarded:  28 \n",
      "\n",
      "NN & SGD:  Matches where 10 points awarded:  109\n",
      "Benchmark: Matches where 10 points awarded:  222 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #For 45 epochs\n",
    "    for epoch in range(40):\n",
    "        avgCost = 0\n",
    "        \n",
    "        #For each feature/label combo in the training set\n",
    "        for i in range(len(X_train)):\n",
    "           \n",
    "            #Set the current feature/label combo\n",
    "            x, y = X_train[i: i + 1], y_train[i: i + 1]\n",
    "            \n",
    "            #Call the Loss function and AdagradOptimizer for it\n",
    "            _, cost = sess.run([optimize, cross_entropy], feed_dict={X: x, Y: y})\n",
    "            \n",
    "            #Calculate average cost so we can monitor what the NN is up to\n",
    "            avgCost+= cost / len(X_train)\n",
    "            \n",
    "        print(\"Epoch:\", (epoch+1), \"| Cost:\", avgCost)\n",
    "\n",
    "    #Find the predicted value for all X_test values by calling sess.run on the softmax of output node\n",
    "    output = tf.nn.softmax(logits)\n",
    "    pred = sess.run(output, feed_dict={X: X_test})\n",
    "    \n",
    "    #If there is only one 1 in the vector we should double it inorder to indicate both teams scored that amount\n",
    "    benchmark = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in benchmark]\n",
    "    y_test = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in y_test]\n",
    "    \n",
    "    \n",
    "    points, temp = [0,0]\n",
    "    score_dict = dict({0: 0, 2: 0, 5: 0, 10: 0})\n",
    "    \n",
    "    \n",
    "    #Calculate the score for the NN & SGD model\n",
    "    for i in range(len(pred)):\n",
    "        p = pred[i]\n",
    "        t = y_test[i]     \n",
    "        s = SGD_test[i]\n",
    "\n",
    "        #One last way of incorporating the SGD predictions was to\n",
    "        #average them into the neural network output and then round off\n",
    "        #the answer to the nearest whole number\n",
    "        #The import thing is not the actual number but the magnitude of it in comparison the rest\n",
    "        final = np.mean(np.array([p, s]), axis = 0)\n",
    "        final = [round(elem) for elem in final]\n",
    "        \n",
    "        #If we have 3+ nonzero numbers, we need to get down to 1 or 2\n",
    "        #This problem was discussed in the SGD and seems to be a byproduct of using\n",
    "        #the average.\n",
    "        while(np.count_nonzero(final) not in [0, 1, 2]):\n",
    "             final[np.random.choice(np.nonzero(final)[0])] = 0\n",
    "        \n",
    "        #If any of the newly created vectors is all zeroes just use the original\n",
    "        #Neural network output instead\n",
    "        for ind in range(len(final)):\n",
    "            if(np.count_nonzero(final[ind]) == 0):\n",
    "                final = p\n",
    "                \n",
    "        #Only 1 index means both teams scored that amount so we must double it\n",
    "        final = [2*elem if np.count_nonzero(elem) == 1 else elem for elem in final]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Perform the Scoring\n",
    "        temp = score_func(final, t)\n",
    "        score_dict[temp] +=1\n",
    "        points += temp\n",
    "    \n",
    "   \n",
    "    points2, temp2 = [0,0]\n",
    "    score_dict2 = dict({0: 0, 2: 0, 5: 0, 10: 0})\n",
    "    \n",
    "    #Calculate the score for the benchmark model\n",
    "    for t, p in zip(y_test, benchmark):\n",
    "           \n",
    "        #Perform the Scoring\n",
    "        temp2 = score_func(p, t)\n",
    "        score_dict2[temp2] +=1\n",
    "        points2 += temp2\n",
    "\n",
    "    \n",
    "    #Display Results\n",
    "    print(\"----------------\")\n",
    "    print(\"Total Score\")\n",
    "    print(\"----------------\")\n",
    "    print(\"NN & SGD:  \", points)\n",
    "    print(\"Benchmark: \", points2)\n",
    "    print(\"----------------\")\n",
    "    print(\"Stats: \")\n",
    "    for k in score_dict.keys():\n",
    "        print(\"NN & SGD:  Matches where\", k, \"points awarded: \", score_dict[k])\n",
    "        print(\"Benchmark: Matches where\", k, \"points awarded: \", score_dict2[k], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The combined NN & SGD model was still unable to surpass the benchmark model. It did however make a huge improvement compared to only the SGDClassifier. The original difference between the benchmark and SGDClassifier was 1515 points, now with the NN & SGD model it is only 765. Described differently the SGDClassifier model had 66.76% of the points of the benchmark, while the final model had 83.20% of the benchmark. The benchmark scores are slightly different between the two comparisons because there were fewer points avaiable in this most recent test as some halftime match data was not available. \n",
    "\n",
    "The only category where the NN & SGD model was able to surpass the benchmark was the 2 point category (where only 1 score matched the true result). Unfortunately, the most important categories for predicting the number of goals or the precise score correctly were dominated by the benchmark model. I tried a variety of methods to improve this (detailed in the report), however the main conclusion I reached is that for this problem, doubling the halftime score is an incredibly good benchmark. It was able to predict the precise correct fulltime score in 15.21% of matches. That's an incredibly high rate for such a simple statistic! In fact, 49.42% of the total points collected by the benchmark were earned that way. In comparison, only 29.30% of the NN & SGD model's points were collected through a precise fulltime score prediction. Maybe I will use the doubled halftimescore as part of the model the next time I approach this problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
